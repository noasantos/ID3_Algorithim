{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3: Algorithim Implementation from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main-Class\n",
    "This class is responsable for holding some common attributes between the Attribute Node(internal) and the Leaf Node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, attributeValue, count):\n",
    "        self.attributeValue = attributeValue\n",
    "        self.count = count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-class: Attribute\n",
    "This subclass extends from Node and initializes with specific attributes related to attribute handling within the tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attribute(Node):\n",
    "    def __init__(self, attribute=None, isDiscreteType=None, majorTargetValue=None, attributeValue=None, count=None):\n",
    "        super().__init__(attributeValue, count)\n",
    "        self.attribute = attribute \n",
    "        self.isDiscreteType = isDiscreteType\n",
    "        self.majorTargetValue = majorTargetValue\n",
    "        self.children = []\n",
    "    \n",
    "    def addLeafNode(self, attributeValue, targetValue, count):\n",
    "        child = Leaf(attributeValue, targetValue, count)\n",
    "        self.children.append(child)\n",
    "        return child\n",
    "    \n",
    "    def addAttributeNode(self, attribute, isDiscreteType, majorTargetValue, attributeValue, count):\n",
    "        child = Attribute(attribute, isDiscreteType, majorTargetValue, attributeValue, count)\n",
    "        self.children.append(child)\n",
    "        return child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-class: Leaf\n",
    "This subclass inherits from Node and is designed to represent leaf nodes in the tree structure, focusing on storing target values associated with attribute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf(Node):\n",
    "    def __init__(self, targetValue, attributeValue, count=None):\n",
    "        super().__init__(attributeValue, count)\n",
    "        self.targetValue = targetValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `build_tree()`\n",
    "This function constructs a decision tree using the ID3 algorithm, based on a given dataset (dataframe). It recursively builds the tree structure, starting from a root node (parentNode) and considering both discrete and continuous attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTree(dataframe: pd.DataFrame, parentNode=None, uniqueValues=None):\n",
    "    target, attributes = dataframe.iloc[:, -1], dataframe.iloc[:, :-1]\n",
    "    bestAttribute, bestSplit = getMostInformativeFeature(attributes, target)\n",
    "    isDiscreteType = True if bestSplit is None else False\n",
    "    \n",
    "    if attributes.empty:\n",
    "        return\n",
    "    elif parentNode is None: #rootNode need to be created\n",
    "        parentNode = Attribute(bestAttribute, isDiscreteType, target.value_counts().idxmax(), attributeValue=None, count=len(attributes))\n",
    "        uniqueValues = {col: dataframe[col].unique().tolist() for col in dataframe}\n",
    "    else:\n",
    "        parentNode.attribute = bestAttribute\n",
    "        parentNode.isDiscreteType = isDiscreteType\n",
    "    \n",
    "    if isDiscreteType: #Column Type is Discrete\n",
    "        currentValues = attributes[bestAttribute].unique()\n",
    "        allAttributeValues = uniqueValues[bestAttribute]\n",
    "        discreteChildren(dataframe, parentNode, currentValues, uniqueValues)\n",
    "        leafForValueNotInCurrValues(parentNode, currentValues, allAttributeValues)\n",
    "    else: #Column Type is Continuous\n",
    "        continuousChildren(dataframe, parentNode, bestSplit, uniqueValues)\n",
    "        \n",
    "    return parentNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `continuousChildren()`\n",
    "\n",
    "This function handles the creation of child nodes for a continuous attribute split in a decision tree, based on a given dataset (dataframe), parent node (parentNode), best split value (bestSplit), and unique attribute values (uniqueValues).\n",
    "\n",
    "The continuousChildren function divides the dataset (dataframe) based on a continuous attribute (parentNode.attribute) into two subsets (dfLessEqual and dfGreater) using the bestSplit value. It then creates child nodes under parentNode for each subset, representing the split condition (<= bestSplit and > bestSplit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuousChildren(dataframe, parentNode, bestSplit, uniqueValues):\n",
    "    dfLessEqual = (dataframe.loc[dataframe[parentNode.attribute] <= bestSplit]).drop(columns=parentNode.attribute)\n",
    "    dfGreater = (dataframe.loc[dataframe[parentNode.attribute] > bestSplit]).drop(columns=parentNode.attribute)\n",
    "    for childDataFrame in [dfLessEqual, dfGreater]: \n",
    "        if childDataFrame.equals(dfLessEqual):\n",
    "            attributeValue = \"<=\" + str(bestSplit)\n",
    "        else:\n",
    "            attributeValue = \">\" + str(bestSplit)\n",
    "        createNewNode(parentNode, childDataFrame, attributeValue, uniqueValues)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `discreteChildren()` and `leafForValueNotInCurrValues()` \n",
    "This function creates child nodes under a parent node (parentNode) for each unique discrete attribute value (currentValues) in a dataset (dataframe), using the ID3 algorithm.\n",
    "\n",
    "The discreteChildren function iterates through each unique discrete attribute value (attributeValue) in currentValues for parentNode.attribute within dataframe. It creates a child node for each subset of dataframe corresponding to attributeValue, using createNewNode to construct nodes based on uniqueValues.\n",
    "\n",
    "Then, the second function adds leaf nodes to a parent node (parentNode) for attribute values that are not present in the current discrete attribute values (currentValues), but are part of the original dataset, ensuring completeness in the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discreteChildren(dataframe, parentNode, currentValues, uniqueValues):\n",
    "    for attributeValue in currentValues:\n",
    "            childDataFrame = (dataframe[dataframe[parentNode.attribute] == attributeValue]).drop(columns=parentNode.attribute)\n",
    "            \n",
    "            createNewNode(parentNode, childDataFrame, attributeValue, uniqueValues)\n",
    "    return\n",
    "\n",
    "def leafForValueNotInCurrValues(parentNode, currentValues, allAttributeValues):\n",
    "    for attributeValue in allAttributeValues:\n",
    "        if attributeValue not in currentValues:\n",
    "            parentNode.addLeafNode(attributeValue, parentNode.majorTargetValue, 0)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNewNode(parentNode, childDataFrame, attributeValue, uniqueValues):\n",
    "    counts = childDataFrame.iloc[:, -1].value_counts()\n",
    "    count = len(childDataFrame)\n",
    "    if isNodePure(counts) or not haveAttributesAvailable(childDataFrame):\n",
    "        targetValue = counts.idxmax()\n",
    "        parentNode.addLeafNode(targetValue, attributeValue, count)\n",
    "    else:\n",
    "        attributeNode = parentNode.addAttributeNode(attribute=None, isDiscreteType=None, count=count, majorTargetValue=counts.idxmax(), attributeValue=attributeValue)   \n",
    "        buildTree(childDataFrame, attributeNode, uniqueValues)\n",
    "    return\n",
    "\n",
    "def haveAttributesAvailable(childDataFrame):\n",
    "    return childDataFrame.shape[1] > 1\n",
    " \n",
    "def isNodePure(counts):\n",
    "    return len(counts) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `entropy()`\n",
    "\n",
    "The entropy function calculates the entropy of a target variable, which is a measure of disorder in the data. It is used extensively in decision tree algorithms for assessing the purity of subsets of data. This universal function serves as the core entropy calculation used across different algorithms(continuous and discrete). It ensures consistency and accuracy in entropy computations throughout the decision tree construction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(target: pd.DataFrame):\n",
    "    counts = target.value_counts()\n",
    "    probs = counts/len(target)\n",
    "    return np.sum(-probs * np.log2(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `continousEntropy()`\n",
    "\n",
    "The continousEntropy function calculates entropy for continuous or numerical features by evaluating potential split points. For Split Entropy Calculation we divide the feature into subsets based on split points and calculates the weighted entropy of each subset using the universal entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continousEntropy(feature, target):\n",
    "    dfTemp = pd.concat([feature, target], axis=1)\n",
    "    entropyResults = {}\n",
    "    inverseEntropyResults = {}\n",
    "    uniqueValuesList = sorted(feature.unique().tolist())\n",
    "    \n",
    "    for value in uniqueValuesList:\n",
    "        totalWeightedEntropy = splitEntropy(dfTemp, value)\n",
    "        \n",
    "        entropyResults[value] = totalWeightedEntropy\n",
    "        inverseEntropyResults[totalWeightedEntropy] = value\n",
    "    \n",
    "    smallestEntropy = min(entropyResults.values())\n",
    "    splitPoint = inverseEntropyResults[smallestEntropy]\n",
    "    return smallestEntropy, splitPoint\n",
    "            \n",
    "def splitEntropy(df: pd.DataFrame, splitPoint):\n",
    "    feature, target = df.columns\n",
    "    dfLessEqual = df.loc[df[feature] <= splitPoint]\n",
    "    dfGreater = df.loc[df[feature] > splitPoint]\n",
    "    \n",
    "    lessEqualEntropy = entropy(dfLessEqual[target])\n",
    "    greaterEntropy = entropy(dfGreater[target])\n",
    "    n = len(df)\n",
    "    totalWeightedEntropy = lessEqualEntropy * len(dfLessEqual)/n + greaterEntropy * len(dfGreater)/n\n",
    "    return totalWeightedEntropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `discreteEntropy()`\n",
    "\n",
    "The discreteEntropy function calculates entropy for discrete or categorical features by evaluating each unique value recursively. It computes the weighted average entropy somatory across subsets formed by each unique value of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  discreteEntropy(feature, target):\n",
    "    dfTemp = pd.concat([feature, target], axis=1)\n",
    "    uniqueValues = feature.unique()\n",
    "    totalWeightedEntropy = 0\n",
    "    \n",
    "    for value in uniqueValues:\n",
    "        subSet = dfTemp[dfTemp[feature.name] == value]\n",
    "        \n",
    "        totalWeightedEntropy += len(subSet) / len(dfTemp) * entropy(subSet[target.name])\n",
    "    \n",
    "    return totalWeightedEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `informationGain()`\n",
    "\n",
    "The informationGain function calculates the information gain when splitting a dataset based on a specific feature. It also evaluates whether a feature is numeric or categorical. If numeric, it computes the weighted entropy after splitting using continuousEntropy, also returning the optimal split point. For categorical features, it computes the weighted entropy using discreteEntropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informationGain(feature, target, entrophyBefore):\n",
    "    continuousSplitPoint = None\n",
    "    if pd.api.types.is_numeric_dtype(feature):\n",
    "        weightedEntrophyAfter, continuousSplitPoint= continousEntropy(feature, target)\n",
    "    else: \n",
    "        weightedEntrophyAfter = discreteEntropy(feature, target)\n",
    "    infoGain = entrophyBefore - weightedEntrophyAfter\n",
    "    return infoGain, continuousSplitPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `getMostInformativeFeature()`\n",
    "\n",
    "The getMostInformativeFeature function identifies the most informative feature for splitting a dataset based on the highest information gain. This function iterates through each attribute in the dataset (attributes) to compute the information gain using informationGain. It compares each attribute's information gain to determine the attribute that provides the highest gain (maxInfoGain). The function returns the best attribute (bestInfoAttribute) and its associated split point (bestSplit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostInformativeFeature(attributes, target):\n",
    "    entrophyBefore = entropy(target)\n",
    "    maxInfoGain = -1\n",
    "    bestInfoAttribute = None\n",
    "    \n",
    "    for _, attributeCol in enumerate(attributes):\n",
    "        colInfoGain, splitPoint = informationGain(attributes[attributeCol], target, entrophyBefore)\n",
    "        if colInfoGain > maxInfoGain:\n",
    "            maxInfoGain = colInfoGain\n",
    "            bestInfoAttribute = attributeCol\n",
    "            bestSplit = splitPoint\n",
    "    return bestInfoAttribute, bestSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `predictClass()` and `addPredictedColumn`\n",
    "\n",
    "This function recursively traverses the decision tree starting from the root node (node). It checks if the current node is a leaf node (Leaf instance); if so, it returns the target value of the leaf. Otherwise, it iterates through the children of the node to find the appropriate child node based on the value of the attribute in the row (rowData). Depending on whether the attribute is discrete or continuous, it compares or evaluates the attribute values to determine the next node to traverse until a leaf node is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictClass(node, rowData):\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.targetValue\n",
    "    \n",
    "    for child in node.children:\n",
    "        if node.isDiscreteType:\n",
    "            if child.attributeValue == rowData[node.attribute]:\n",
    "                return predictClass(child, rowData)\n",
    "        else:\n",
    "            if child.attributeValue.startswith(\"<=\"):\n",
    "                if rowData[node.attribute] <= float(child.attributeValue.lstrip(\"<=\")):\n",
    "                    return predictClass(child, rowData)\n",
    "            elif child.attributeValue.startswith(\">\"):\n",
    "                if rowData[node.attribute] > float(child.attributeValue.lstrip(\">\")):\n",
    "                    return predictClass(child, rowData)\n",
    "\n",
    "\n",
    "def addPredictedColumn(rootNode, testData: pd.DataFrame):\n",
    "    predicted = []\n",
    "    \n",
    "    for _, row in testData.iterrows():\n",
    "        predictedClass = predictClass(rootNode, row)\n",
    "        predicted.append(predictedClass)\n",
    "    testData['Predicted Class'] = predicted\n",
    "    return testData     \n",
    "\n",
    "def calculateAccuracy(testData):\n",
    "    return accuracy_score(testData.iloc[:, -2], testData.iloc[:, -1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `runID3_MODE()`\n",
    "\n",
    "The functions runID3_TRAINTEST and runID3_ALLDATA both implement the ID3 algorithm but serve different purposes. runID3_TRAINTEST evaluates model performance by splitting data into training and testing sets, building a decision tree on the training data, predicting test set labels, and assessing accuracy. . In contrast, runID3_ALLDATA trains the model on the entire dataset and focuses solely on visualizing the decision tree structure, providing a comprehensive view of the decision-making process without validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runID3_TRAINTEST(datasource):\n",
    "    df = preProcess(pd.read_csv(datasource, keep_default_na=False)) #ESSA PARTE AQUI SOLUCIONA O PROBLEMA NO NaN, ELE DEIXA TUDO COMO VEM\n",
    "    trainData, testData = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    rootNode = buildTree(trainData)\n",
    "    printTree(rootNode, attributeChild=False)\n",
    "    testData = addPredictedColumn(rootNode, testData)\n",
    "    print(testData)\n",
    "    buildTreeImg(rootNode, \"iris\")\n",
    "    return calculateAccuracy(testData)\n",
    "\n",
    "def runID3_ALLDATA(datasource):\n",
    "    df = preProcess(pd.read_csv(datasource, keep_default_na=False))\n",
    "    rootNode = buildTree(df)\n",
    "    printTree(rootNode, attributeChild=False)\n",
    "    \n",
    "    buildTreeImg(rootNode, \"iris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `buildTreeImg()`\n",
    "\n",
    "The function buildTreeImg and its helper function exportTree are designed to visually represent a decision tree using the Graphviz library, primarily for graphical purposes. buildTreeImg initializes a Digraph object from Graphviz and calls exportTree with the root node of the decision tree (rootNode). It then exports the resulting graph to a PNG file named according to the dataset name (dataset_name). The function also opens a viewer to display the tree graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTreeImg(rootNode, dataset_name):\n",
    "    dot = exportTree(rootNode)\n",
    "    dot.render(\"f{dataset_name}_ID3_DT\", format=\"png\")\n",
    "    dot.view()\n",
    "\n",
    "def exportTree(node, dot=None, parent_name=None, edge_label=\"\"):\n",
    "    if dot is None:\n",
    "        dot = Digraph()\n",
    "    \n",
    "    if isinstance(node, Attribute):\n",
    "        node_label = f\"{node.attribute}\"\n",
    "    elif isinstance(node, Leaf):\n",
    "        node_label = f\"{node.targetValue}\\n{node.count}\"\n",
    "    \n",
    "    curr_node_name = f\"{id(node)}\" \n",
    "    dot.node(curr_node_name, label=node_label, shape=\"ellipse\" if isinstance(node, Leaf) else \"box\")\n",
    "    \n",
    "    if parent_name is not None:\n",
    "        dot.edge(parent_name, curr_node_name, label=edge_label)\n",
    "    \n",
    "    for child in node.children:\n",
    "        edge_label = child.attributeValue\n",
    "        exportTree(child, dot, curr_node_name, edge_label)\n",
    "                   \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `preProcess()`\n",
    "\n",
    "The preProcess function preprocesses a Pandas DataFrame (dataframe) by setting the index to the 'ID' column if it exists. This operation helps organize the data for easier access and manipulation, especially in scenarios where each row can be uniquely identified by an ID.\n",
    "\n",
    "falar mais e pensar o que mais fazer com o preProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(dataframe: pd.DataFrame):\n",
    "    if 'ID' in dataframe.columns:\n",
    "        dataframe.set_index('ID', inplace=True)   \n",
    "    return dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
